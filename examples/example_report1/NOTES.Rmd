---
title: KDB ELN
author: "Matthew Ralston <mrals89@gmail.com>"
header-includes:
  \usepackage{fancyhdr}
  \usepackage{graphicx}
  \AtBeginDocument{\let\maketitle\relax}
  \newcommand{\beginsupplemental}{\setcounter{table}{0}\renewcommand{\thetable}{S\arabic{table}} \setcounter{figure}{0} \renewcommand{\thefigure}{S\arabic{figure}}}
output: pdf_document

bibliography: bibliography.bib
---


\makeatletter
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\thepage}
  \fancyhead[L]{\Large \textbf{\@title} \\ \large \@author}
  \fancyhead[R]{\href{https://matthewralston.github.io}{Analyst}}
}

\pagestyle{plain}
\vspace*{1\baselineskip}





```{r include=F, message=F, echo=F, warning=F}
set.seed(1234)
gc(verbose = getOption("verbose"), reset = FALSE, full = TRUE)



library('ggplot2')
library('scales')
library('fitdistrplus')
library('DBI')
library('RSQLite')
library('tidyr')
library('sitools')



###################################
#  F u n  c t i o n s
###################################
median.quartile <- function(x){
    out <- quantile(x, probs = c(0.25,0.5,0.75))
    names(out) <- c("ymin","y","ymax")
    return(out) 
}

bottom.quartile <- function(x){
    out <- quantile(x,probs=c(0.25))
    return(out)
}

top.quartile <- function(x){
    out <- quantile(x,probs=c(0.75))
    return(out)
}

getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

```




## Objectives

1. Is the best fit independent of k?
2. What parameters optimize the profile generation time?
3. How does the profile generation time scale with k?
4. Can the correlation distance recapitulate phylogenetic knowledge?
5. If so, what depth is asymptotically similar to 



# Abstract



# Document Purpose

In contrast to the CHANGELOG.md, this file is a log (an ELN of sorts) of my thought process through debugging or improving specific calculations or models. It is ordered chronologically, as the questioned occurred, rather than reflecting specific timings of implementation or organized in a way that reflects interactions/separations in functions. It's a human and scientific document, not a integration testing document.


# Introduction

The machine I use is configured like a fairly light-weight blade. It's got a 4-core i5-4440 @ 3.30 Ghz, 16Gb memory, and running Kernel 5.9.14-arch1-1. It does have onboard graphics that are Radeon, so sadly no cuda support yet. But it can do certain alignment tasks and it can certainly handle a few twelve-mers, but not a dozen. And since the dataset I am working with in the other report is centered upon 11 species, I must keep that in mind when doing the most expensive (in terms of memory) operation in the workflow: matrix operations. These matrix operations such as dimensionality reduction, distance calculation, normalization, etc must all be done with the rank determined by the number of databases being held in memory (not sparsely, interestingly enough), and the dimensionality the primary rate-limiting parameter, k, a choice made to optimize both the memory restrictions and the storage restrictions on profile sizes. 

I have implemented sparse profiles in storage, but not in memory. I don't feel it is necessary to accomodate such memory restrictions in order to properly create insights with standard metrics and toolkits, instead of custom implementations of various normalizations, regularizations, distances, dimensionality reduction, it's too much to implement custom, so instead we just make the best product we can using standard numpy, scipy, matplotlib, pandas etc. and let the users know about the memory limitations. So this is your guide to being able to make the most, when you have access to a machine or server without the memory limitations.

## What is the maximum choice of k?
With consumer hardware, kdb profile generates 8-mer profiles without issue. In principle, the command should scale as both memory and storage read/write speeds increase, and the command can generate the needed on-disk SQLite3 databases with nearly 140Tb worth of data, or a maximum choice of k of 22 @ 70.368744177664 TB with a 32bit integer resolution (a maximum of 4 billion per cell). If we switched to 64 bit integers it would double the amount of storage to nearly 140.737488355328 TB but now with a maximum k-mer count of nearly 18 pentillion per cell. In reality, we'll still need to choose something smaller in the end because most filesystems wont support files of that size. BTRFS supports a maximum file size of 50Tb, which wouldn't hold a choice of k of 22 at 32 bit resolution, let alone 64bit integer resolution. So we reduce k a little to 20, or about 4Tb per file at 32 bit integer resolution or 8Tb per file at 64 bit integer resolution. Note that when I say per file i mean per SQLite file, which would likely exceed this with metadata. If we include nearest neighbor information (4 neighbors x 2 strand directions) then we multiply the size of needed storage by perhaps an order of magnitude at best case, orders of magnitude at worst case.

Certainly with an eye to the future, we can see that the solution prototyped here can scale to at least k=20 with on-disk counting and without choosing an optimal database for rapid counting, and if "rapid" is even necessary if it's only done once. The tool was used to generate 8-mer profiles in this study, and we will be restricting the scope of k to choices of k from 8-12 for simplicity and focus on addressing 4 immediate deliverables.

1. Compute time scaling of profile subcommand under different choices of `-k` and `-p $CPU`
2. Profile fidelity (again) from k=8-12 with different choices of subsampling size.
3. File size compressed and decompressed vs k.
4. Process memory for 8-12-mers x 11 sample matrix

# Methodology

The advent of Next-Generation Sequencing technology may be one of the most influencial technologies of the early 21st century. 

```{r fig.cap="Primary histogram showing the discrete distribution of count data"}
ints <- c(2, 4, 8)
#summary(kmers$count) # Repetitive
k <- seq(8, 25)
perm <- matrix(4^k)

data <- data.frame(matrix(perm) %*% t(ints))
colnames(data) <- c("16 bit", "32 bit", "64 bit")
data$k <- k
data <- gather(data, "label", "bytes", -k)
ggplot(data) + geom_line(aes(x=k, y=bytes, colour=label)) + scale_y_log10("Array size [Bytes]", breaks=c(1e6, 1e9, 1e12, 1e15), labels=f2si) + ggtitle("Array size")

```



# Methodology


# Results





An Amazon Linux instance was started in the US-East-1 availability zone. 



## Building k-mer profiles from fastq data sacrifices accuracy

A streaming inter-profile distance metric was implemented to assess similarities between profiles. The correlation coefficient was chosen as a self-normalizing metric to assess the differences between a profile generated from second-generation sequencing compared to its reference genome.

A dataset was generated from the *C. acetobutylicum* ATCC824 genome with `art_illumina` and sampled at various depths with `fastq-tools` to understand how well sequencing datasets could reflect the true k-mer profile that can be derived from its reference genome(@huang2012art, @jones2015fastqtools). As shown in Fig 4., increasing k tends to decrease similarity between the sequenced dataset and the reference genome, reflected by the correlation coefficient of the profiles. 


It can be stated that whereever possible, k-mer profiles from reference genomes should be utilized for inferences. This could be an artifact introduced during the subsampling routine used when generating the simulated WGS dataset. However, the reference genome represents a condensed and unbiased estimate of the consensus assembly and should be used for the reference k-mer profile, when a reference is available. Additionally, in some cases sufficient sequencing depths are not available to accurately reflect the reference genome. In WGS and assembly applications, between 10-30x is advised to ensure even and minimum coverage across chromosomes. When such depths are not available, lower choices of k can be used to make basic, simple inferences about sequences with k-mer profiles.


An Amazon Linux instance was started in the US-East-1 availability zone. 


### Prepare Amazon Linux AMI for parameter sweep

```bash
sudo yum update
sudo yum install tmux git
sudo yum install python3
sudo yum groupinstall "Development Tools"
mkdir pckges && cd pckges
wget ftp://ftp.pcre.org/pub/pcre/pcre-8.44.zip
wget https://ftpmirror.gnu.org/parallel/parallel-20200222.tar.bz2
bzip2 -dc parallel-20200222.tar.bz2 | tar xvf -
git clone https://github.com/dcjones/fastq-tools
# unzip, ./configure, make, make install the above
cd
git clone https://github.com/MatthewRalston/kmerdb.git
cd kdb && sudo pip3 install -r requirements.txt
```

### Parameter sweep

Now that we have examined the performance of generating profiles and matrices within 8 < k < 12, we must look at the sensitivity possible when looking at idealized fastq error behavior through the lens of `art_illumina`, a Illumina read simulator made in the National Institute of Environmental Health Sciences' Biostatistics Branch.


```bash
# Generate a representative and oversequenced sample with basic indel error profile
# Simulating HiSeq 2500x series reads, readlength of 150, 100x coverage
art_illumina -ss HS25 -i $FASTA -l 150 -f 10000 -o single_out

subsample(){
  s=$1
  k=$2
  #echo "Running in $(pwd)" >&2
  sample=$(mktemp tmp.XXXX)
  echo "Subsampling ${s} reads into ${k}-mer profile: ${sample}" >&2
  fastq-sample -n $s -s $RANDOM single_out.fq -o $sample
  /home/matt/Projects/kdb/bin/kdb profile -k $k $sample.fastq $sample.kdb
  corr=$(/home/matt/Projects/kdb/bin/kdb distance correlation $sample.kdb $FASTA.kdb)
  echo -e "${k}\t${s}\t${corr}" >> Cac_subsample_correlations.txt
  rm $sample $sample.fastq $sample.kdb
}
export -f subsample
parallel -j $CPU 'subsample {1} {2}' ::: $(seq 10000 40000 800000) ::: $(seq 8 12) ::: $(seq $CPU)
```


```{r fig.cap="Correlation distance between subsampled profiles and reference genome"}

# R code to generate histogram from parameter sweep
corr_to_ref<-read.table("Cac_subsample_correlations.txt", header=F)
colnames(corr_to_ref) <- c("k", "Reads", "Correlation")

ggplot(corr_to_ref) + geom_point(aes(x=Reads, y=Correlation, colour=k))

```


### Secondary parameter sweep

```bash
# Generate representative but oversequenced samples from each of the 11 genomes with basic indel error profile
# Simulating HiSeq 2500x series reads, readlength of 150, 100x coverage
parallel -j 3 'gunzip {} && art_illumina -ss HS25 -i {.} -l 150 -f 1000 -o {.}.master' ::: $(/bin/ls test/data/*.fasta.gz)
rm test/data/*.master.aln # Remove the unnecessary .aln files
parallel -j 3 'gzip {}' ::: $(/bin/ls test/data/*.master.fq) #Compress the data before moving to SSD storage
# If you have ample storage, you can remove the compression step and decompression/recompression steps from the following routine
subsample(){
  s=$1
  k=$2
  f=$3
  f=$(basename $f .fasta)
  gunzip $f.master.fq.gz
  #echo "Running in $(pwd)" >&2
  sample=$(mktemp tmp.XXXX)
  echo "Subsampling ${s} reads into ${k}-mer profile: ${sample}" >&2
  fastq-sample -n $s -s $RANDOM single_out.fq -o $sample
  /home/matt/Projects/kdb/bin/kdb profile -k $k $sample.fastq $sample.kdb
  corr=$(/home/matt/Projects/kdb/bin/kdb distance correlation $sample.kdb $FASTA.kdb)
  echo -e "${k}\t${s}\t${corr}" >> subsample_correlations.txt
  gzip $f.master.fq
  rm $sample $sample.fastq $sample.kdb
}
export -f subsample
parallel -j $CPU 'subsample {1} {2} {3}' ::: $(seq 1000000 4000000 500000) ::: $(seq 8 12) ::: $(/bin/ls test/data/*.fasta)
```




## Supplemental

\beginsupplemental

The following is incomplete

### K-mer counting program runtime vs k

The most relevant question that needed answering early in the analytical process was how long the profile generation time could be expected to take between fastq datasets generated via `art_illumina` vs ideal k-mer profiles generated from fasta files of sequenced organisms. 

From initial inspections, it seems like generating k-mer count profiles from BioPython SeqRecord objects streamed from fastq files requires considerable calculation time. This could be improved by using an alternative fastq parser library for Python that would read only the sequence information into memory. Additionally, a data fastq-fasta preprocessing step could lessen the amount of parsing and object creation and thus GC overhead experienced by the program, which may be a factor reducing efficiencies.

We explored how the run time varied with respect to the choice of k on a fixed number of 250k pairs of reads subsampled from the *C. acetobutylicum* RNA-Seq dataset SRR1774150. By increasing k, we were able to investigate the tradeoff between the spectrum's sensitivity to species specific k-mers (k) and the average runtime required for a specific sensitivity. 

Additionally, the number of processing cores had a mild effect at reducing processing time. 


#### Alternate histogram of 10-mer distributions











## Bibliography




