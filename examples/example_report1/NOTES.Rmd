---
title: KDB ELN
author: "Matthew Ralston <mrals89@gmail.com>"
header-includes:
  \usepackage{fancyhdr}
  \usepackage{graphicx}
  \AtBeginDocument{\let\maketitle\relax}
  \newcommand{\beginsupplemental}{\setcounter{table}{0}\renewcommand{\thetable}{S\arabic{table}} \setcounter{figure}{0} \renewcommand{\thefigure}{S\arabic{figure}}}
output: pdf_document

bibliography: bibliography.bib
---


\makeatletter
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\thepage}
  \fancyhead[L]{\Large \textbf{\@title} \\ \large \@author}
  \fancyhead[R]{\href{https://matthewralston.github.io}{Analyst}}
}

\pagestyle{plain}
\vspace*{1\baselineskip}





```{r include=F, message=F, echo=F, warning=F}
set.seed(1234)
gc(verbose = getOption("verbose"), reset = FALSE, full = TRUE)



library('ggplot2')
library('scales')
library('fitdistrplus')
library('DBI')
library('RSQLite')
library('tidyr')
library('sitools')



###################################
#  F u n  c t i o n s
###################################
median.quartile <- function(x){
    out <- quantile(x, probs = c(0.25,0.5,0.75))
    names(out) <- c("ymin","y","ymax")
    return(out) 
}

bottom.quartile <- function(x){
    out <- quantile(x,probs=c(0.25))
    return(out)
}

top.quartile <- function(x){
    out <- quantile(x,probs=c(0.75))
    return(out)
}

getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

```




## Objectives

1. Is the best fit independent of k?
2. What parameters optimize the profile generation time?
3. How does the profile generation time scale with k?
4. Can the correlation distance recapitulate phylogenetic knowledge?
5. If so, what depth is asymptotically similar to 



# Abstract



# Document Purpose

In contrast to the CHANGELOG.md, this file is a log (an ELN of sorts) of my thought process through debugging or improving specific calculations or models. It is ordered chronologically, as the questioned occurred, rather than reflecting specific timings of implementation or organized in a way that reflects interactions/separations in functions. It's a human and scientific document, not a integration testing document.


# Introduction

With consumer hardware, kdb profile generates 8-mer profiles without issue. In principle, the command should scale as both memory and storage read/write speeds increase, and the command can generate the needed on-disk SQLite3 databases with nearly 140Tb worth of data, or a maximum choice of k of 22 @ 70.368744177664 TB with a 32bit integer resolution (a maximum of 4 billion per cell). If we switched to 64 bit integers it would double the amount of storage to nearly 140.737488355328 TB but now with a maximum k-mer count of nearly 18 pentillion per cell. In reality, we'll still need to choose something smaller in the end because most filesystems wont support files of that size. BTRFS supports a maximum file size of 50Tb, which wouldn't hold a choice of k of 22 at 32 bit resolution, let alone 64bit integer resolution. So we reduce k a little to 20, or about 4Tb per file at 32 bit integer resolution or 8Tb per file at 64 bit integer resolution. Note that when I say per file i mean per SQLite file, which would likely exceed this with metadata. If we include nearest neighbor information (4 neighbors x 2 strand directions) then we multiply the size of needed storage by perhaps an order of magnitude at best case, orders of magnitude at worst case.

Certainly with an eye to the future, we can see that the solution prototyped here can scale to at least k=20 with on-disk counting and without choosing an optimal database for rapid counting, and if "rapid" is even necessary if it's only done once. The tool was used to generate 8-mer profiles in this study, and we will b



The advent of Next-Generation Sequencing technology may be one of the most influencial technologies of the early 21st century. 

```{r fig.cap="Primary histogram showing the discrete distribution of count data"}
ints <- c(2, 4, 8)
#summary(kmers$count) # Repetitive
k <- seq(8, 25)
perm <- matrix(4^k)

data <- data.frame(matrix(perm) %*% t(ints))
colnames(data) <- c("16 bit", "32 bit", "64 bit")
data$k <- k
data <- gather(data, "label", "bytes", -k)
ggplot(data) + geom_line(aes(x=k, y=bytes, colour=label)) + scale_y_log10("Array size [Bytes]", breaks=c(1e6, 1e9, 1e12, 1e15), labels=f2si) + ggtitle("Array size")

```



# Methodology


# Results




## Outstanding concerns

Many issues remain in the formulation of the probability calculation, addressed below. Additionally, the calculation of sequence profiles from streamed fastq files is taking a considerable amount of time. Each individual read might only contain a small number of kmers, but the object creation and streaming libraries used make the streaming of individual reads slow. Generating a profile from even 50k 150bp reads take longer than the correlation calculation does.





## K-mer count distribution

We begin with the k-mer count distribution, a graphical analysis to help us understand if the $4^{k}$ k-mers' count distribution is in agreement with existing k-mer distributions in the literature. This ELN does not claim to address the question of "which distribution is most suitable to model k-mer counts" but instead offers the distribution to illustrate what background model might be appropriate for modeling efforts based on individual k-mer counts taken from k-mer spectra, like those generated by this software. Below is a simple histogram generated from retention of the intermediate SQLite3 database. The intermediate database is an exact replica of the k-mer database bgzf file (.kdb), and is used here because no bgzf parser exists for R. The k-mers table consists of all $4^{k}$ k-mer counts and is the basis for the histogram (Fig 1.).



```{r fig.cap="Primary histogram showing the discrete distribution of count data"}
library(DBI)

con <- dbConnect(RSQLite::SQLite(), "../test/data/Cacetobutylicum_ATCC824.8.sqlite3")
dbListTables(con)
kmers <- dbReadTable(con, "kmers")
dbDisconnect(con)

#summary(kmers$count) # Repetitive
ggplot(kmers) + geom_histogram(aes(x=count)) + ylab("K-mers") + xlab("Counts") + ggtitle("8-mer  counts of C. acetobutylicum ATCC824")

```


The k-mer profile is visualized here as a histogram to illustrate the distribution of counts that any k-mer may have. In addition to the summary statistics presented below, the mode is `r getmode(kmers$count)`. Interestingly, `r length(kmers[kmers$count > 500,1])` 8-mers occur more than 500 times in the *C. acetobutylicum* genome. These sequences likely represent homopolymers, regions where misassemblies are likely to occur, and potentially repetitive regulatory motifs.



By generating the histogram and associated skewness-kurtosis analysis (@cullen1999probabilistic graph, Fig 2.), we can ask ourselves whether a Poisson model is appropriate, or if alternatives are more appropriate for modeling probabilities of counts of specific k-mer features associated with a genome. Though the negative-binomial seems more appropriate as suggested by the R package `fitdistrplus`, the kurtosis is still more extreme than would be required for an ideal fit in the NB model (@delignette2015fitdistrplus).


```{r fig.cap="Cullen and Frey analysis of skewness and kurtosis suggesting best fit"}
descdist(kmers$count, discrete=T, boot=20)
```

```{r fig.cap="Alternate histogram showing Poisson model (blue) and Negative binomial fit (red)"}
hist(kmers$count, prob=T, breaks=200, main="Histogram of k-mer counts", xlab="Kmer counts")
poisson_fit <- fitdist(kmers$count, 'pois')
poisson_fitD <- dpois(0:max(kmers$count), lambda=poisson_fit$estimate)
lines(poisson_fitD, lwd="3", col="blue")
nbinom_fit <- fitdist(kmers$count, 'nbinom')
nbinom_fitD <- dnbinom(0:max(kmers$count), size=nbinom_fit$estimate[1], mu=nbinom_fit$estimate[2])
lines(nbinom_fitD, lwd="3", col="red")
nbinom_fit$estimate
```

To illustrate this further, Fig 3. shows us the two competing discrete distributions suggested by `fitdistrplus`. In blue, a Poisson model is shown to be a poor fit of the existing count data on the histogram from Fig 1. In contrast, a negative-binomial fit(red) with size = `r nbinom_fit$estimate[1]` and $\mu$ = `r nbinom_fit$estimate[2]` provides a reasonable fit for the dataset. The negative binomial model is a canonical discrete model often used in the RNA-Seq and k-mer literature to model count data like those obtained through second-generation sequencing experiments (@anders2010differential, @daley2013predicting).

In summary, the k-mer count distribution is best approximated by a negative-binomial model. The k-mer counts/frequencies and their distribution could be used to model sequence likelihoods via Markov probabilities. Other applications of k-mer probabilities will be explored below.


## Building k-mer profiles from fastq data sacrifices accuracy

A streaming inter-profile distance metric was implemented to assess similarities between profiles. The correlation coefficient was chosen as a self-normalizing metric to assess the differences between a profile generated from second-generation sequencing compared to its reference genome.

A dataset was generated from the *C. acetobutylicum* ATCC824 genome with `art_illumina` and sampled at various depths with `fastq-tools` to understand how well sequencing datasets could reflect the true k-mer profile that can be derived from its reference genome(@huang2012art, @jones2015fastqtools). As shown in Fig 4., increasing k tends to decrease similarity between the sequenced dataset and the reference genome, reflected by the correlation coefficient of the profiles. 


It can be stated that whereever possible, k-mer profiles from reference genomes should be utilized for inferences. This could be an artifact introduced during the subsampling routine used when generating the simulated WGS dataset. However, the reference genome represents a condensed and unbiased estimate of the consensus assembly and should be used for the reference k-mer profile, when a reference is available. Additionally, in some cases sufficient sequencing depths are not available to accurately reflect the reference genome. In WGS and assembly applications, between 10-30x is advised to ensure even and minimum coverage across chromosomes. When such depths are not available, lower choices of k can be used to make basic, simple inferences about sequences with k-mer profiles.


An Amazon Linux instance was started in the US-East-1 availability zone. 


### Prepare Amazon Linux AMI for parameter sweep

```bash
sudo yum update
sudo yum install tmux git
sudo yum install python3
sudo yum groupinstall "Development Tools"
mkdir pckges && cd pckges
wget ftp://ftp.pcre.org/pub/pcre/pcre-8.44.zip
wget https://ftpmirror.gnu.org/parallel/parallel-20200222.tar.bz2
bzip2 -dc parallel-20200222.tar.bz2 | tar xvf -
git clone https://github.com/dcjones/fastq-tools
# unzip, ./configure, make, make install the above
cd
git clone https://github.com/MatthewRalston/kdb.git
cd kdb && sudo pip3 install -r requirements.txt
```

### Parameter sweep

```bash
# Generate a representative and oversequenced sample with basic indel error profile
# Simulating HiSeq 2500x series reads, readlength of 150, 100x coverage
art_illumina -ss HS25 -i $FASTA -l 150 -f 10000 -o single_out

subsample(){
  s=$1
  k=$2
  #echo "Running in $(pwd)" >&2
  sample=$(mktemp tmp.XXXX)
  echo "Subsampling ${s} reads into ${k}-mer profile: ${sample}" >&2
  fastq-sample -n $s -s $RANDOM single_out.fq -o $sample
  /home/matt/Projects/kdb/bin/kdb profile -k $k $sample.fastq $sample.kdb
  corr=$(/home/matt/Projects/kdb/bin/kdb distance correlation $sample.kdb $FASTA.kdb)
  echo -e "${k}\t${s}\t${corr}" >> Cac_subsample_correlations.txt
  rm $sample $sample.fastq $sample.kdb
}
export -f subsample
parallel -j $CPU 'subsample {1} {2}' ::: $(seq 10000 40000 800000) ::: $(seq 8 12) ::: $(seq $CPU)
```


```{r fig.cap="Correlation distance between subsampled profiles and reference genome"}

# R code to generate histogram from parameter sweep
corr_to_ref<-read.table("../data/Cac_subsample_correlations.txt", header=F)
colnames(corr_to_ref) <- c("k", "Reads", "Correlation")

ggplot(corr_to_ref) + geom_point(aes(x=Reads, y=Correlation, colour=k))

```



### Building k-mer profiles from fastq data takes more time




### Streaming distance metric calculations

 Given that some choices of k may be too extreme to load the vector into memory, a streaming implementation of each distance metric is necessary. Some metrics will necessarily require only one pass, such as the un-normalized Euclidean distance metric. Other metrics may require multiple passes over each file to generate arithmetic averages that may be used in indvidual components of the summation.


The first metric to implement was the correlation distance. This distance metric is described first to illustrate a single-pass metric with a streaming implementation, which is necessary to support large values of k. In early implementations I have written have the summation as essentially:

$r = ssxy/\sqrt{ssxx \cdot ssyy}$

$ssxy_i = (freq_x - mean_x)(freq_y - mean_y)$
$ssxx_i = (freq_x - mean_x)^2$
$ssyy_i = (freq_y - mean_y)^2$

For each element i of the $4^{k}$ k-mer profile, the residuals are calculated and added to a running sum, which is then used to calculate the final correlation coefficient as shown.





## Supplemental

\beginsupplemental

The following is incomplete

### K-mer counting program runtime vs k

The most relevant question that needed answering early in the analytical process was how long the profile generation time could be expected to take between fastq datasets generated via `art_illumina` vs ideal k-mer profiles generated from fasta files of sequenced organisms. 

From initial inspections, it seems like generating k-mer count profiles from BioPython SeqRecord objects streamed from fastq files requires considerable calculation time. This could be improved by using an alternative fastq parser library for Python that would read only the sequence information into memory. Additionally, a data fastq-fasta preprocessing step could lessen the amount of parsing and object creation and thus GC overhead experienced by the program, which may be a factor reducing efficiencies.

We explored how the run time varied with respect to the choice of k on a fixed number of 250k pairs of reads subsampled from the *C. acetobutylicum* RNA-Seq dataset SRR1774150. By increasing k, we were able to investigate the tradeoff between the spectrum's sensitivity to species specific k-mers (k) and the average runtime required for a specific sensitivity. 

Additionally, the number of processing cores had a mild effect at reducing processing time. 


#### Alternate histogram of 10-mer distributions











## Bibliography




