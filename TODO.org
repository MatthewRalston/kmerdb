# Kmer DB

.kdb files should be debrujin graph databases
The initial prototype would be plaintext
The final prototype would be .bgzf format from biopython





* DONE Sparse .kdb
  CLOSED: [2021-01-07 Thu 21:37]
** modify slurp
** modify profile
* DONE Nearest neighbor profile
  CLOSED: [2021-01-07 Thu 21:37]
* VERIFY index class
* VERIFY Probability function
* Rmd report1
** Introduction
** Methodology
** Results
*** Distribution fitting / model selection
*** PCA
*** 

** Distribution analysis
** Accurately describe kdb counting algorithm
*** althought the algorithm differs in its approach to fastq k-mer counting from fasta k-mer counting,
*** First, a selection of sequences is shredded into k-mers in memory
*** Second, the counts are tallied on-disk using SQLite3.
*** Third, the SQLite3 database iterator is used to pull row from row out and print line by line into the kdb datastructure.
*** Fourth, at this point, an index may be created.
** Distribution fitting
*** Cullen-Frey
*** Negative binomial fit
*** Poissonian imitation (average, geom. mean, median, mode) [each] vs negative binomial fit to the data
** Count normalization
*** Next, we want to judge the effect of DESeq2 normalization on the counts values.
*** We use a boxplot to address the null-hypothesis that DESeq2 normalization does not meaningfully harmonize each samples quartiles with one another.
*** We must check this often when addressing our normalized data because failure to normalize properly
*** due to an issue that is not library size or total counts, 
*** suggests another issue with the distribution of that sample.
*** State why we refuse to standardize the data at this point.

* Rmd report2
** algorithm profiling
** kdb profile k x time x cpu (z)
*** we need to choose a range of k that is meaningful and explain why.
*** the choice of k of 8 - 12 is convenient because it means
*** we don't have to pay for extra memory. This will be managable on any number of cores
*** with at least 32 Gb of memory for about 20 samples.
*** According to the following graph, the uncompressed value of the sparse matrix in n x 4^k
*** may take gigabytes per profile in the low double digits.
*** but the value of these profiles grows exponentially with the increased cost as well.
*** so when we look at these genomes with this degree of sensitivity, which has been substantial in the literature in the neighborhood of k=10-12,
*** then suddenly we agree that more characterizations are possible and this places more value on the expected scaling behavior of this program.
*** The goal is most likely not to reinvent the wheel. Since this is an academic package at this point, we feel that it is necessary and important to couple this with a graph database
*** We have selected the RDF format going forward and expect that long term use of Amazon Neptune might be an important source of understanding that we can get from users uploading their graphs, sparse or otherwise, to a giant Neptune repository.
*** It could be an entirely new sequence database format.
** kdb distance correlation <fasta|fastq>
* TODO profile reads sam/bam
** use pysam to iterate over reads, creating a profile in the process.
* Genome size estimation 
* Likelihood of dataset given prior k-mer profiles
* Calculate graph properties indicative of de Bruijn graph collapse


* 'kmerdb random' sequence simulator
** given a certain length of sequence N, suggest a sequence that best solves the k-mer abundance graph

* kmerize
** to use bed/gff features to select reads from bam/bai using pysam
** and then creating sparse profiles for each feature
** to split a bam according to gff/bed features, and putting that in an output directory
* AWS Nepture / rdflib / Berkley DB / MongoDB support
** Learn the RDF spec
** Think of a specification for each node.
* Manifold learning
** Isomap (derived from multidimensional scaling (MDS) or Kernel PCA)
*** Lower dimensional projectsion of the data preserving geodesic distances between all points
** (Modified) Locally Linear Embedding
*** Lower dimensional projection of the data preserving local neighborhood distances
*** locally_linear_embedding or LocallyLinearEmbedding with method="modified"
** t-SNE
*** While isomap, LLE, and variants are best tuited to unfold a single continuous low-dimensional manifold
*** t-SNE will focus on the local structure of the data and will tend to extract clustered local groups of samples.
*** This ability to group samples based on the local structure might be beneficial to visually disentangle a dataset that comprises several manifolds at once.

* TODO Comment code
* index class
** need b-tree library
*** https://pythonhosted.org/BTrees/
** input dictionary
*** given a int/float I want fast access to all keys greater than or less than the int/float
*** e.g. { 345: [line offsets], 346: [lineoffsets} sorted by the int/float
*** The following searches for all values greater-than(min) or less-than(max), flattening
*** list(itertools.chain.from_iterable(btree.values(min=int/float)))
* kdb annotator class (reworked into index class and better metadata specification)
*** TODO First, further specify kdb record shape
*** TODO Second specify kdb metadata shape/types/parsing routines
*** Annotate bools, floats (probability), tags, ints (connectivity/degree)
**** Eulerian as a tag or a bool?
*** Index should be designed to rapidly filter tags, rapidly search/filter/narrow on ints
* Index function
** kmer id index : parse header offset (done?), then use readline + .tell() to get offset
** count index : b-tree
*** sort k-mers by counts (in memory, not on file), then create b-tree, leafs are k-mer file indices (above)
** tag : hash index
** float, int indices : similar to count index above6
* Priorities
** Connect this to meme suite
** Hypotheses:
*** Suppose that k-mer spectra have a positive and negative saturation direction.
*** In this way, more specific signals and antisignals could be surmissed from samples with enough resolution, temporal or otherwise resolved by covariates. 
*** Think of what could happen if the signals and antisignals were resolved on the order of genes, you could detect gene expression levels with it.
* Operations
** DONE Get all neighbors
   CLOSED: [2019-11-12 Tue 14:41]
*** Remove first/last letter, add one of the 3 other possible letters
*** 6 possible neighbors
** is_terminal = True if all neighbors of one direction have 0 count
** Eulerian walk (Maybe at the Python level and not the C-api)
*** Return a group of k-mers that have a complete walk

* DONE Format specification
  CLOSED: [2019-12-02 Mon 13:40]
** YAML header (first block) 
*** format version
*** choice of k
*** file name, sha256 checksums, number of reads, kmers added
*** comments
kdb_ver: 0.0.1
k: 14
files:
  - filename: 
    sha256: 
    md5: 
    total_reads: 
    total_kmers: 
    unique_kmers: 
  - filename: ...
comments:
** kmers (other blocks)
*** kmer id
*** count (exclude 0 count kmers?)
*** yaml metadata/neighboring k-mer ids
* toolkit
** DONE Reverse strand
   CLOSED: [2019-12-02 Mon 13:39]
** DONE utility functions
   CLOSED: [2019-12-02 Mon 13:39]
*** DONE translate kmers to/from binary encoding
    CLOSED: [2019-10-30 Wed 12:14]
*** DONE header validation
    CLOSED: [2019-11-12 Tue 14:32]
** DONE summary
   CLOSED: [2019-10-30 Wed 12:14]
*** print information from header
** DONE profile
   CLOSED: [2019-12-02 Mon 13:38]
*** VERIFY new profile is sum of individual profiles
**** for x in range(len(f.profile)):
****     final.profile[x] += f.profile[x]
*** closed
**** DONE kdb.file.checksums generates checksums of a file
     CLOSED: [2019-11-06 Wed 02:25]
**** DONE prof=array.array('H'); for x in range(4**k): prof.append(0)
     CLOSED: [2019-11-06 Wed 02:26]
**** DONE prof[sequenceToBinary(kmer)] += 1
     CLOSED: [2019-11-06 Wed 02:26]
**** DONE total_kmers += 1
     CLOSED: [2019-11-06 Wed 02:26]
**** DONE total_reads += 1
     CLOSED: [2019-11-06 Wed 02:26]
**** DONE unique_kmers = 4**k - prof.count(0)
     CLOSED: [2019-11-06 Wed 02:26]
**** DONE support multiple files
     CLOSED: [2019-11-12 Tue 14:31]
**** DONE generate streaming profile (file or [[https://gist.github.com/MatthewRalston/6641f45bdce19341f568264132b794de][S3 download to temp]])
     CLOSED: [2019-11-12 Tue 14:32]
**** DONE KDBReader.read_profile 
     CLOSED: [2019-11-12 Tue 14:31]
**** DONE KDBWriter.write_profile
     CLOSED: [2019-11-12 Tue 14:31]
** VERIFY similarity
*** cumulative formulas
**** these need to be calculated differently for efficiency/memory reasons
**** repetitive summation/multiplication and not direct to unit vector transformation
**** DONE 1. Pearson correlation coefficient of counts? of unit vector?
     CLOSED: [2019-11-07 Thu 13:03]
**** DONE 2. euclidean distance of unit vectors?
     CLOSED: [2019-11-07 Thu 13:03]
**** 3. sort by count of vector/index and Spearman
*** jaccard
**** presence/absence (k-mer is observed in both profiles? it's in the intersection
**** similar count within a tolerance... vs Spearman?
*** MUMi distance
** jsonify
*** transform the debrujin graph into json
** Partitioning experiment
*** Use khmer to partition reads from an example dataset
*** Similarity metrics between partition fastas and whole profile
*** Annotate kdb metadata to include Markov probabilities of single sequences to partition
*** How do we describe or select subgraphs based on the partition information?
**** Presence of Eulerian walk among partition AND if the eulerian walk extends too far into other partitions
**** Key reads AND k-mers involved in complex graph structures near partition bridges
**** Suggestions for deeper sequencing or skew in partition compositions to make up for low depth
**** Number of partition bridges vs subsampling
**** Number of partition bridges vs unique k-mer count / partition
**** Other metrics besides unique k-mer count
***** Overlap k-mer count
***** unique k-mers per total k-mers
***** unique k-mers per partitioned reads
*** How do we describe subgraph features worth considering, given the partition
**** Node connectivity stats
**** kdb filtering ( retrieve only k-mers with partition, connectivity, Markov probability cutoffs, participant in Eulerian walk)
** Other functions
*** Partitionizer (partition fasta and genomic fastas; completeness of each partition's capture of the ideal composite)
**** How much more data do I need from each partition to minimize bridges, maximize genomic coverage, and maximize orthogonality to other partitions
**** Given a partition fasta and a genomic fasta
**** Could estimate the sequencing depth and complexity required to minimize *most* partition bridges
**** Could also estimate the size and partitioning required to maximize partition orthogonality
*** Sampleizer (one genome fasta; dial up/back efforts in improving this partition/sampling)
**** Does my sampling protocol for this partition only have enough uniqueness to cover the one major walk, or is most of the data getting in the way of the other species at the current composite compositions?
**** How much of the genomic profile is covered by the partition?
**** At a certain orthogonality metric per sampling from the genomic fasta, does the amount of uniqueness orthogonality recovered by additional depth tend to clarify the partition, or obfuscate other operations on leading partitions?
*** Profilizer (all genome fastas; snapshot/metrics, as composite is improved)
**** Construct a perfect profile from all genomes and integrate
**** Similarities between individual profiles and perfect composite (Ideal distance metrics for each profile addition to perfect the composite)
**** Similarities between imperfect composite and perfect composite (How much orthogonality and completeness is currently recovered)
**** Similarities between imperfect partitions and perfect composite (How much orthogonality is lost due to current imperfect partitioning)
**** Similarities between imperfect composite and imperfect partitions (How much orthogonality is lost due to current imperfect partitioning)
*** walker (calculate Eulerian walks, i.e. walks that maximize path length under constrains (no node visited twice, etc.))
**** it's an optimization of some kind
**** under the constraint of 'no node visited twice'
**** maximize walk length (like the number of joins)
* Other functions
** chimera, duplications, transposon, contamination detection (kPAL)
** [[https://kpal.readthedocs.io/en/latest/method.html#distance-metrics][multiset distance/similarity (kPAL)]]
** Peak detection and modality analysis (single k-mer peak, low neighbors? broad k-mer abundance peaks?)
** k-mer spectrum plotting (ggplot? tsv?)
** sequencing error vs rare k-mer likelihoods (Kelley et all 2010 https://genomebiology.biomedcentral.com/articles/10.1186/gb-2010-11-11-r116)
** kdb filter for repetitive motifs/sequences?? 
** replace header (kdb header replace example.kdb example.yaml)
*** Leaving the count fields at 0 is okay, should recompute anyway
*** If the count fields are non-zero, then assume the values are correct

* Report
** How does sparseness scale linearly with the choice of k
** What is the appropriate distribution for k-mer counts
** Vanila (no-metadata) Profile generation time
*** Runtime vs reads (fasta, fastq)
*** Runtime vs filesize 
*** Compare slopes from regression to determine if profiles can be generated from fasta files faster
** How do profiles from WGS, simulated Illumina reads, and the assembled genome differ?
** Is there good separation Markov-chain probabilities of sequences from different species against a profile?
