
#+TITLE: KmerDB (.kdb)  [TURN THAT TSHI UP]
#+SUBTITLE: A simple bgzf-based k-mer database format
#+AUTHOR: Matthew Ralston <mralston.development@gmail.com>

# .kdb files should be debrujin graph databases
# The final prototype would be .bgzf format from biopython

* Objectives
** Rename functions
*** DONE edges of the 1st order de Bruijn graph
CLOSED: [2025-07-12 Sat 21:16]
:LOGBOOK:
- State "DONE"       from "FEEDBACK"   [2025-07-12 Sat 21:16]
- State "FEEDBACK"   from "DONE"       [2025-07-09 Wed 14:46]
- State "DONE"       from "FEEDBACK"   [2025-07-09 Wed 14:46]
- State "FEEDBACK"   from "WAITING"    [2025-07-09 Wed 14:46]
- State "WAITING"    from "IN-PROGRESS" [2025-07-09 Wed 14:46]
- State "IN-PROGRESS" from "NEXT"       [2025-07-09 Wed 14:46]
:END:
** edges of the 2nd order de Bruijn graph
** convert to networkx

* Dates
** 7/12/24
Over the last 5-7 days, I've been reworking the kmer.py and graph.py modules. Associated function invocations in parse.py have also been modified. The goal of the refactoring included interface changes in functions and a solution for ambiguous k-mer edge relationships via ambiguous IUPAC residues.

The next step is to make the edge relationships between the k+1 mers. This next step involves turning the edge relationships of a .kdbg file (or a new accessory function in graph.py) into a k+1mer node list, and then edges from that list, given positional information.

Again the goal is an edge list, but this one should be the final edge list of interest? I think... I hope.

** 7/6/25  | 0.9.0
After a little bit of time working on things, I've acceptance tested the kmerdb graph and profile commands from this refactor. The refactoring was mostly housekeeping in the modules graph.py, kmer.py, and parse.py.

The end result is a modified graph format, which represents the edges of the original de Bruijn graph. It uses greedy neighbor joining to reduce the numbers of edges present in the primary de Bruijn graph, and consequently the number of nodes in the secondary graph, considering the k+1 mers (or kmer-to-kmer adjacencies) as nodes.

I'm happy with the result, and I still need
** 7/5/24  | kmer.py smell
Wow... I just took a look at how bad kmer.py looks right now. There are a good number of comments, but I believe the code is considerably ugly at the moment.
I understand some of it is just legacy and smells now. But the kmer.Kmer.shred() and kmer.Kmer.shred_for_graph methods are under considerable amount of redundancy and smell at the time.

I suggest a rewrite. It's a nice module and it's a shame that it's so well documented but poorly tasteful.

In some regards, the Kmer class isn't even that well suited for parallelization. I kind of wish I could rewrite everything.

First I should rewrite the kmer module. The kmer_to_id and id_to_kmer functions are fine. I need to rewrite shred to have a simpler interface and not be part of this stupid Kmer class I had to make to facilitate multiprocessing.

Parallelism is not the priority here. I need to get rid of the bloated kmer.py module and seqparser modules pretty much entirely.

The parse.py module is mostly okay. A little distasteful but mostly okay at the moment. kmer.py really needs the rewrite.

kmerdb.parse.parsefile()L152 could use a rewrite along with the related functionality in kmer.py. Ideally the kmer.shred functionality should just produce a list of kmer_ids rather than a list of dictionaries AT ALL.

Problem encountered in kmer.py rewrite: If a non-standard IUPAC character is detected in the sequence, the kmer_ids array that comes out of shred will not have length equal to N-k+1.

Proposed solution? Return both a randomly selected kmer from the doublet/tripled expanded by the non-standard (not ATCG) 

Second problem encounterd in kmer.py rewrite: If a N character is detected then we cannot/don't include that kmer_id (as it is None) in the kmer_ids

Proposed solution: Include None in the kmer_ids array as a placeholder. This is also an option when replacing doublets/triplets in the shred function when keyword arg 'replace_with_none' is True.

1pm : Refactored parse.py and kmer.py. Still need to work on graph.py/graphlib. Took maybe 2 hours. Reworking _profile to be cleaner and removed the 'dtype' arguments. It's a flat file. Doesn't need this metadata.

12pm : Refactored the graph.py into a new file. Produces an edge list of 2-tuples of (i, kmer_id, neighbor_id) where i is the id of the k-mer amongst all k-mers in the accumulated data

kmer_id is the current_kmer, and neighbor_id is a kmer_id.



** 7/4/24  | Assembly algorithm
I'm kind of thinking that the assembly algorithm would be much more fun to do than the alignment algorithm and gives me a chance to showcase more in-depth abilities with coding and problem solving.
So prior to this I had been thinking about PyDot/GraphViz and networkx as components of a potential assembly algorithm.

I think first pass, I should implement a BFS linear-algebra strategy to compute the powers of the adjacency matrix product of interest to modulate the frontier vector. According to Anthropic's Claude Sonnet 4:

- Start with an initial binary vector v_{0} representing your source nodes.
- Each BFS level corresponds to multiplying by the adjacency matrix: v_{1} = A*v_{0},   v_{2} = A*v_{1} ... etc.
- The non-zero entries in the corresponding v_{k} indicate which nodes are reachable in k steps.

The algorithm can have significantly different traversal strategies depending on the choice of k. One study (can't remember, may be the one that follows or at least referenced in it) found that small k often leads to better results with so-called 'irregular' graphs. That have node-degree distributions that are considered more complex.
Basically, irregular graphs have a few highly connected nodes which make searching from these nodes more complex and time consuming on steps with choice k that are large.

Again, from Claude Sonnet 4:

This matrix perspective reveals that BFS is essentially computing powers of the adjacency matrix A^{k} to find all nodes reachable within k steps.

*** TurboBFS and COO/CSC format based multiplication on GPU

The authors Artiles and Saeed of the paper of interest refer to a 'TurboBFS' algorithm for BFS that can/is implemented on the GPU.
The core component of the innovation outline in the paper is the use of certain data structures for minimal memory footprint that can easily be ported to VRAM on the GPU.

They have a GitHub repo at https://github.com/pcdslab/TurboBFS that contains C/C++ and CUDA components for a command-line implementation of the TurboBFS search strategy (which is top-down)
and the memory efficient data structures known as Compressed Sparse Column (Yale) and Coordinate format (row, col, val) for a sparse graph and multiplication functions that work on these data structures.

Quite simply, they exclude matrix elements that have values of 0, and store the sparse matrix in a simpler vector with length = # non-zero matrix elements.

More interestingly, they have multiplication functions that work on these representations. I'm assuming that's in their C and CUDA code.


As such they may take steps through undirected graphs to produce the 'frontier' vector as the start points lead to the resolution of the BFS when the frontier becomes nulled/zeroed.

NOTE: The authors in this paper mention on p4. that their implementation omits the 'value' column of the COO(C) format completely to save memory. If the adjacency matrix A of the graph is binary, then only the row-indices and column indices are needed to create the multiplication with the frontier vector f_{k}




*** Steps towards implementation
- 1. Change the edge-list output to a true tkmer_id -> neighbors such that there are tot_kmers = N-k+1 (nodes) in the output and 8 *tot_kmer edges
- 2. SIMPLE PyDot/networkx export compatability with the 'kmerdb graph' function. (work on ammending the graph format specification at a later point)
- 3. Work on Eulerian path on k-mer pairs, instead of the nodes being a k-mer the node is a kmer-to-kmer relationship, the edges of the original problem. Now the Eulerian walk is possible (ty to charlesreid1.github.io)
- 4. The Eulerian path algorithm may involve some BFS. Let's work on a naive implementation around networkx before the linear algebra is needed.
- 5. Convert NetworkX into an adjacency matrix
- 6. Implement a dummy COO or CSC class with conversion utilities to-from networkx for export to PyDot
- 7. Change this into a cuSPARSE representation
- 8. Create a BFS search method using numpy matrix multiply
- 9. Verify BFS search strategy on simple sequence for modest k
- 10. Implement a TurboBFS strategy on the COO/CSC data structures and cuSPARSE
- 11. Turn this into CUDA/cuSPARSE instructions. Might need Rust support?





** 6/28/25 | Alignment algorithm
I'm revisiting the minimizers and alignment algorithm. Doing some tidying on the topic of alignment and minimizers by virtue of first revisiting the 'minimizer.py'  module.
It's pretty cluttered. I'm disappointed kind of. But the minimizers functionality should be easy to fix, and the alignment should be a low hanging fruit afterwards.

I'd like to read the vsearch paper a little bit more. My current plant is to essentially load the compacted minimizers index into memory at once. I may or may not include data during the read method where the is_min column is 0.
Essentially just read the sequence id, coordinate on the sequence, the kmer_id, and the is_min value. This will be done for both the temporary .kdbi index on the queries (also on a temporary .kdb file I assume). I don't need to build intelligence in here yet about asssuming there is some kind of .kdb file just lying around with the same basename.

And then that's it. I'm cutting down the size of the minimizers module.


** 6/27/25 | Gene Ontology RefSeq CDS selection and codons refactoring
Working on a method to retrieve organism RefSeq annotations by GO term via NCBI EUtils. Currently not working for several bacterial genomes.
This method also seems to be not useful for UniProt because many bacterial proteins are not appropriately included in the UniProt DB scope.

Also refactoring the codons.py submodule because of usability issues and some incorrect output observed in the command line regarding include/exclude of start/stop codons.
Reinvestigating through refactoring. Will make sure that codons runs correctly before revisiting the outcomes from the chisquare test in CUB due to the challenges in extracting RefSeq IDs matching the Gene Ontology terms.

After putting some work into this, I've found that the Blazegraph SPARQL endpoint isn't returning results from the query I've been running.
I need an alternate method to access Gene Ontology terms, and I've settled on using AmiGO to get Gene Ontology terms associating with genes.


** 6/21/25 | [0.8.19] CUB added, still in progress

I found several things that *didnt* work. First, when codon frequencies are equal to 1 (i.e. when observed codon counts for an amino acid for one codon are divided by the expected and this equals 1), the chisquare test returns NaN
I also added some options to include/exclude start/stop codon counts in the table, or to totally refuse stop codon counts from the table, and then parse and remedy that same table if the stop codon counts are excluded.
This is still giving inf and NaN results from the chisquare, or at times is just doing 0.0. All results of chisquare so far have been unfruitful.

** 6/14/25 | [0.8.17] codons added. kmer, parse module fixed
Released on PyPI as 0.8.17
Fixed kmer.py module kmer_to_id and id_to_kmer functions, added is_aa amino acid conversions
parse.py module: correct min/max sequence/read lengths added to header. Was putting placeholder 50,0 min max sequence lengths before.
Adds codons.codon_frequency_table() which returns a codon list, codon counts, and freqencies
Adds codons.get_codons_in_order which is a list of 3-mer ids
Both functions make sure L%3 == 0

